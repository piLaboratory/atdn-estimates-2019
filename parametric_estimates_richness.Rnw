\documentclass[12pt, A4]{article}
%\usepackage[brazil]{babel}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{natbib}
\bibliographystyle{rusnat}
\usepackage{framed, color}
\usepackage{xspace}
\definecolor{shadecolor}{rgb}{0.9, 0.9, 0.9}
\newcommand{\R}{\textnormal{\sffamily\bfseries R}\xspace}
\newcommand{\code}[1]{\texttt{#1}}


\title{Parametric estimates of total tree richness in Amazon}
\author{Han Ter Steege, Paulo In√°cio Prado, Renato Lima, ATDN}

\begin{document}

\maketitle


<<setup, echo=FALSE, include=FALSE>>=
library(VGAM)
library(untb)
##library(fitdistrplus)
library(sads)
library(knitr)
##library(SPECIES)
source("functions.R")
opts_chunk$set(fig.align = 'center', fig.show = 'hold',
               fig.height = 6, warning = FALSE, message = FALSE,
               error = FALSE, echo=TRUE)
options(formatR.arrow = TRUE, width = 90, cache=TRUE)
@ %def 

<<Data prep>>=
Dec2018 <- read.csv2("data.csv", as.is=TRUE)
y <- Dec2018$N.ind
Sobs <- length(y)
## Total number of trees (average density x area)
Tot.t <- 567*5.5e8
## Proportion of total trees in the sample
p1 <- sum(Dec2018$N.ind)/Tot.t
## Total number of plots
N.plots <- 1945
@ 

\section*{Poisson lognormal}

Seems to overestimate the abundance of the most abundant species (see qq-plots)
 
<<fit pln>>=
pln <- fitpoilog(y)
par(mfrow=c(2,2))
plot(pln)
par(mfrow=c(1,1))
@ %def 

\subsection*{Species richness estimation from the underlying lognormal}


Still, let's use the estimated coefficients to estimate total species richness.
The estimated parameters of the Poilog are:

<<poilog coefficients>>=
(pln.cf <- coef(pln))
@ %def 

In this model the observed SADs is a Poisson sample of a lognormal SAD of the whole community.
The parameter $\sigma$ of the Poilog estimates the same parameter of the underlying lognormal.
The parameter $\mu$ of the Poilog has the following relationship with the corresponding parameter
of the sampled lognormal ($M$):

$ M = \mu - \ln(p)$

where $p$ is the proportion of the community that has been sampled
\citep{Bulmer1974,Saether2013}.
%% As the total sampled area is $2.048 \times 10^3$ ha and the 
%% total area of Amazon is $5.79 \times 10^8$ ha, $p=$~\Sexpr{(p1=2.048e3/5.79e8)}.
%% Therefore, we have $M=$~\Sexpr{unname(pln.cf[1])-log(p1)}.

As the estimated total number of trees in  Amazon is $3.1 \times 10^{11}$, 
and the number of identified trees in the sample
is \Sexpr{(sum(Dec2018$N.ind))}, $p=$~\Sexpr{p1}.
Therefore, we have $M=$~\Sexpr{unname(pln.cf[1])-log(p1)}.

The mean of the lognormal distribution is 

$E[X] = e^{M + \frac{\sigma^2}{2}}$

And then this model says that the regional SAD is lognormal
with an average number of trees/species of

 
<<mean lognormal>>=
pln.cf <- unname(pln.cf)
Mu <- pln.cf[1]-log(p1)
(mean.ln <- exp(Mu + pln.cf[2]^2/2))
@ %def 

The estimated richness is then the total number of trees in the region
divided by the mean above: 

 
<<lognorm S estimate>>=
## Estimated number of species
(pln.S <- Tot.t/mean.ln)
@ %def 

Rather disappointing. I guess that the problem is the assumption that the pooled samples
can be approximated by a Poisson sample of the entire tree community of Amazon (pretty unrealistic indeed).


\subsection*{Estimating species richness directly from PLN}

An alternative way \footnote{I guess this is the way recommended by developers of the \emph{poilog} package, to be checked} 
to estimate total number of species in the sampled metacommunity
is simply to calculate the value of the Poisson-lognormal fitted to the data at
the zero value. That is, to estimate the probability value assigned to species that
had zero abundance in the sample. Simple as that:

<<PLN species richness estimate 2>>=
(pln.d0 <- dpoilog(0, mu = pln.cf[1], sig=pln.cf[2]))
@ 

That is, an estimate that the recorded species are \Sexpr{round(100-pln.d0*100,0)}\%
of the total number of species.
This gives an estimate of \Sexpr{round(Sobs/(1-pln.d0))} species.

\subsubsection*{Likelihood and confidence intervals}

Confidence profiles for both parameters are quite well-behaved (parabolic)
and so match frequentist confidence intervals:

<<confidence and likelihood limits of PLN>>=
pln.prf <- profile(pln)
par(mfrow=c(1,2))
plotprofmle(pln.prf)
par(mfrow=c(1,1))
(pln.ci <- confint(pln.prf))
(pln.li <- likelregions(pln.prf))
@ 

The likelihood interval of total species richness can be estimated
visually from the likelihood surface. 
Using the likelihood ratio of two (bold black isoline)
we can see the isolines that are tangent tothe likelihood region.
%likelihood interval for total species richness from PLN is ca 5600 -- 5750:

<<PLN richness ci, echo=FALSE>>=
pln.smu <- seq(pln.li$mu[[1]][1], pln.li$mu[[1]][2], length=25)
pln.ssig <- seq(pln.li$sig[[1]][1], pln.li$sig[[1]][2], length=25)
LL1 <- Vectorize( function(mu,sig)
    -sum(sads::dtrunc("poilog", x=y, trunc=0, coef=list(mu=mu, sig=sig), log=TRUE)) )
ER1 <- Vectorize(function(mu,sig) Sobs/ (1 - dpoilog(0, mu,sig)))
pln.ll <- outer(pln.smu, pln.ssig, LL1)
pln.llr <- pln.ll - LL1(pln.cf[1],pln.cf[2])
Sest <- outer(pln.smu, pln.ssig, ER1)
contour(x=pln.smu, y=pln.ssig, z=pln.llr, xlab="mu", ylab="sigma",
        levels=3:5)
contour(x=pln.smu, y=pln.ssig, z=pln.llr, levels=2, add=TRUE, lwd=2, labcex=1.5)
contour(x=pln.smu, y=pln.ssig, z=Sest, add=TRUE, col="blue", lwd=2, labcex=1)
#contour(x=pln.smu, y=pln.ssig, z=Sest, add=TRUE, col="blue", lwd=2, labcex=1,
        #levels= c(5589,5756, round(Sobs/(1-pln.d0))))
@ 

Because confidence intervals of the estimated parameters are very close to likelihood intervals 
we have a similar result using one or another interval to estimate a interval for the total species richness. 
But the likelihood surface help us by showing 
that we have to use the upper bound of $\mu$ CI with the lower bound of $\sigma$ CI
to get the lower bound of species richness (and the opposite for the
upper bound):

<<<pln S ci>>=
## Lower bound of S
Sobs / (1 - dpoilog(0, pln.ci[1,2], pln.ci[2,1]))
## Upper bound
Sobs / (1 - dpoilog(0, pln.ci[1,1], pln.ci[2,2]))
@ 

Still much lower than the estimates provided by log-series or negative binomial.

\section*{Log-series}

Log-series seems to overestimate the number of singletons
and to underestimate the abundance of species with intermediary abundances
(between 16 and 64 individuals, see octave plot). Also, qq-plots show that
the model underestimates the larger abundances.

 
<<fit ls>>=
y.ls <- fitls(y)
par(mfrow=c(2,2))
plot(y.ls)
par(mfrow=c(1,1))
@ %def 

\subsection*{Estimate of species richness}

Well, you have that stuck in your brain, I suppose. 
But just for the records:

 
<<estimated S ls>>=
alpha <- coef(y.ls)[[2]]
(S.ls <- alpha*log(1 + Tot.t/alpha))
@ %def 

And here is the confidence interval for Fisher's $\alpha$ and the
interval for estimated total richness from these values

<<ls and S est for ls>>=
(ls.ci <- confint(y.ls))
## Estimated species richness for lower bound of alpha's IC
ls.ci[1]*log(1 + Tot.t/ls.ci[1])
ls.ci[2]*log(1 + Tot.t/ls.ci[2])
@ 

\section*{Negative binomial}

Here I use the method proposed by \citet{tovo2017}. The first step is
to fit a negative binomial to the abundances in the sample.
I did that with the \emph{VGAM} package (as the authors did)
and also with the \emph{sads} package. 
The results were similar, and the fit looks identical to
those provided by the log-series (but has much lower AIC value, see below).

 
<<fit NB>>=
## With VGAM
y.nb <- vglm(y ~ 1, posnegbinomial)
## With sads
y.nb2 <- fitnbinom(y, start.value=c(size=0.3, mu=mean(y)))
## Comparing: 
exp(coef(y.nb))
coef(y.nb2)
@ %def 

<<nb plots>>=
par(mfrow=c(2,2))
plot(y.nb2)
par(mfrow=c(1,1))
@ 


\subsection*{Estimate of  species richness}

Following the recipe of \citet{tovo2017}:

 
<<Tovo S estimate>>=
cf.nb <- coef(y.nb2)
csi.p <- unname(cf.nb[2]/(sum(cf.nb)))
csi <- csi.p/(p1+(1-p1)*csi.p)
## Estimated number of species 
S.nb <- Sobs*(1-(1-csi)^cf.nb[1]) / (1-(1-csi.p)^cf.nb[1])
(S.nb <- unname(S.nb))
@ %def 

I did a simple function to automate the calculations and to return
the confidence intervals, based on the confidence intervals of the 
coefficients of the NB fit (see file \code{functions.R})

 
<<NB S est CI>>=
(tovo.S <- tovo(fit = y.nb2, p = p1, CI=TRUE))
@ %def 


\section*{Model selection}

Among the three models, the Negative Binomial
provides the best fit:

 
<<model selection>>=
AICtab(pln, y.nb2, y.ls, base=TRUE)
@ %def 

\section*{Regional RAD}

Here I replicated the extrapolation method from the regional RAD of the
estimated abundances, first for the logseries projection:

<<Linear extrapolation from regional RAD>>=
## Regional RAD
pop.rad <- rad(Dec2018$population)
## Linear regression through central 50% quantiles of the RAD
p.lm <- lm(log(abund)~rank, data=data.frame(pop.rad), subset=rank>max(rank)*.25&rank<max(rank)*.75)
## Regression coefficients
cf.p.lm <- unname(coef(p.lm))
## Logseries projection (upper bound)
(S.reg1 <- abs(cf.p.lm[1]/cf.p.lm[2]))
@ 

And also the same extrapolation assuming a log-normal, which is lower-bound estimate:

<<regional rad lognormal>>=
d <- log(max(Dec2018$population))-cf.p.lm[1]
(S.reg2 <- abs((cf.p.lm[1]-d)/cf.p.lm[2]))
@ 

\subsection*{Regional RAD from negative binomial}

Here I compare the regional RAD with RADs predicted by the logseries
and the negative binomial for the upper-bound and lower-bound estimate of species
obtained above. I also added the regression lines used to estimate such lower and
upper bounds


<<nbinom rad>>=
## ppoints for estimated species richness
S.nb.p1 <- rev(ppoints(tovo.S$S.est))
## Predicted Negative Binomial for 100 points over the rad using richness predicted from negbib lower limit mu
## Estimated richness value
nb.pred <- rad.posnegbin(S = tovo.S$S.est, size = cf.nb[1], prob = 1-tovo.csi(cf.nb[1], cf.nb[2], p1))
## Lower and upper limits
nb.pred.l <- rad.posnegbin(S = tovo.S$CIs[4,1], size = tovo.S$CIs[1,1], prob = tovo.S$CIs[3,1])
nb.pred.u <- rad.posnegbin(S = tovo.S$CIs[4,2], size = tovo.S$CIs[1,2], prob = tovo.S$CIs[3,2])

nb.pred1 <- qposnegbin(p=S.nb.p1[seq(1,tovo.S$S.est,length=100)], size = cf.nb[1], prob = 1-tovo.csi(cf.nb[1], cf.nb[2], p1))

nb.pred2 <- qposnegbin(p=S.nb.p1[seq(1,S.reg1,length=100)], size = y.nb2.ci[1,2], prob = 1-y.nb2p.u)
## Logseries for 100 points
#ls.pred1 <- qls(p=S.nb.p1[seq(1,S.reg1,length=100)], N=Tot.t, alpha= a.reg1) ## overflow
## The plots
plot(abund~rank, data=data.frame(pop.rad), xlim=c(1,S.reg1), ylim=c(1,max(Dec2018$population)), log="y")
lines(nb.pred, col="blue")
lines(seq(1,tovo.S$S.est,length=100), nb.pred1, col="blue")
lines(seq(1,S.reg1,length=100), nb.pred2, col="green")
curve(exp(cf.p.lm[1]+cf.p.lm[2]*x), add=TRUE)
@ 



\section*{Parametric estimates from occupancies}

Here goes a quick exploration of models for the distribution of
occupancies (that is, the proportion of plots in which each species
has been recorded). Such distribution captures a bit of
the spatial aggregation of species \footnote{check if what has already been done with these distributions; maybe we can call them SODs (Species Occupancy Distributions). Moreover, following core-satellite hypothesis, occupancies should correlate with abundances and them we might think on modelling a bivariate SAD x SOD distribution.}. 

I tried two tentative approaches.
The first one was to calculate the empirical occupancies (that is, the proportion
of plots in which each species has been recorded) and then I fitted
a beta distribution to these values. 
To take into account undetected species I tried to truncate this
continuous distribution at different points.

The second approach was to fit the distribution
of frequencies (the number of plots out of the total of \Sexpr{N.plots}
in which each species has been recorded) to
a beta-binomial distribution. Because this is a discrete distribution, 
truncation is at a single point (species with zero frequency in the sample).

In both cases I assumed that the occupancies of the species
follow a beta distribution, and that the observed frequencies were
the result of independent \Sexpr{N.plots} trials for each species, in which
the number of occupied plots follow a binomial distribution. 
Hence the compound beta-binomial distribution that underlies both approaches.
With this distribution we can estimate the proportion of undetected species
and then the total number of species.

\subsection*{Truncated Beta distribution}

The beta distribution is used to describe a distribution of bounded
continuous variables like probabilities. We can think observed
occupancies as estimates of the probabilities of occurrence of each species
in a plot. 

Because species with low occupancy values have lower detectability,
the distribution should give more weight for the lower values of occupancies,
as they are underrepresented in the sample. 
A first-order approximation is to truncate the
beta distribution at 1/(number of plots). 

I then fitted the distribution of occupancies to beta distributions
without truncation and truncated at $0.1/$\Sexpr{N.plots}, $0.5/$\Sexpr{N.plots} and $0.9$\Sexpr{N.plots}.
The code for fitting with maximum likelihood
is in the source file of this document.  
Below I show the QQplots of predicted x observed values,
which shows that truncation above $0.5/$\Sexpr{N.plots} clearly
improves the fit:

<<beta fit, echo=FALSE>>=
########################################################
## Fit a beta and truncated beta to observed proportions
########################################################
## Initial fit to get start values
f.st <- fitdist(Dec2018$N.plots/N.plots, "beta")
cf.st <- coef(f.st)
## likelihood functions
LL.beta <- function(s1, s2){
    -sum ( dbeta(x = Dec2018$N.plots/N.plots, shape1=s1, shape2=s2, log=TRUE) )
}
LL.betat.1 <- function(s1, s2){
    -sum ( sads::dtrunc("beta", x = Dec2018$N.plots/N.plots, trunc=0.1/N.plots, coef = list(shape1=s1, shape2=s2), log=TRUE) )
}
LL.betat.5 <- function(s1, s2){
    -sum ( sads::dtrunc("beta", x = Dec2018$N.plots/N.plots, trunc=0.5/N.plots, coef = list(shape1=s1, shape2=s2), log=TRUE) )
}
LL.betat.9 <- function(s1, s2){
    -sum ( sads::dtrunc("beta", x = Dec2018$N.plots/N.plots, trunc=0.9/N.plots, coef = list(shape1=s1, shape2=s2), log=TRUE) )
}
## Fits: ran first with SANN and the with default method
f.beta <- mle2(LL.beta, start=list(s1=cf.st[1], s2=cf.st[2]))
f.betat.1 <- mle2(LL.betat.1, start=list(s1=cf.st[1], s2=cf.st[2]), 
                  method="SANN")
f.betat.1 <- mle2(LL.betat.1, start=as.list(coef(f.betat.1)))
f.betat.5 <- mle2(LL.betat.5, start=list(s1=cf.st[1], s2=cf.st[2]), 
                  method="SANN")
f.betat.5 <- mle2(LL.betat.5, start=as.list(coef(f.betat.5)))
f.betat.9 <- mle2(LL.betat.9, start=list(s1=cf.st[1], s2=cf.st[2]), 
                  method="SANN")
#f.betat.9 <- mle2(LL.betat.9, start=as.list(coef(f.betat.9))) ## did not converge
## Coefficients
(cf.beta <- coef(f.beta))
(cf.betat.1 <- coef(f.betat.1))
(cf.betat.5 <- coef(f.betat.5))
(cf.betat.9 <- coef(f.betat.9))
@ 

<<beta qqplots, echo=FALSE>>=
## Checking fits with QQ-plots
par(mfrow=c(2,2))
QQ.plot(Dec2018$N.plots/N.plots, distr = "beta",
        coef = list(shape1 = cf.beta[1], shape2 = cf.beta[2]),
        main="No Truncation")
QQ.plot(Dec2018$N.plots/N.plots, distr = "beta", trunc=0.1/N.plots,
        coef = list(shape1 = cf.betat.1[1], shape2 = cf.betat.1[2]),
        main=paste("Truncation at 0.1/",N.plots, sep=""))
QQ.plot(Dec2018$N.plots/N.plots, distr = "beta", trunc=0.5/N.plots,
        coef = list(shape1 = cf.betat.5[1], shape2 = cf.betat.5[2]),
        main=paste("Truncation at 0.5/",N.plots, sep=""))
QQ.plot(Dec2018$N.plots/N.plots, distr = "beta", trunc=0.9/N.plots,
        coef = list(shape1 = cf.betat.9[1], shape2 = cf.betat.9[2]),
        main=paste("Truncation at 0.9/",N.plots, sep=""))
par(mfrow=c(1,1))
@

And the model selection shows that the truncation at $0.9/$\Sexpr{N.plots}
provides a much better fit:

<<beta model selection>>= 
AICtab(f.beta, f.betat.1, f.betat.5, f.betat.9)
@ 

\subsubsection*{Estimate of species richness}

<<Estimated number of species, echo=FALSE, include=FALSE>>=
## Estimated number of species using the beta binom truncated
(bb.S <- bb.Sest(shape1=cf.beta[1], shape2=cf.beta[2], Sobs=S.obs, size=N.plots))
(bbt.1.S <- bb.Sest(shape1=cf.betat.1[1], shape2=cf.betat.1[2], Sobs=S.obs, size=N.plots))
(bbt.5.S <- bb.Sest(shape1=cf.betat.5[1], shape2=cf.betat.5[2], Sobs=S.obs, size=N.plots))
(bbt.9.S <- bb.Sest(shape1=cf.betat.9[1], shape2=cf.betat.9[2], Sobs=S.obs, size=N.plots)) ## unrealistic high
@

If we assume that species occur in the plots
independently with different values of occupancies
(described by the beta distribution fitted above),
the distribution of frequencies (number of occupied plots) 
follows a \textbf{beta-binomial distribution}. Because species that
have not occurred in any plot are undetected, 
the beta-binomial distribution
of observed frequencies is truncated at zero. 
The full distribution nevertheless returns the probability
of zero frequency, which estimates the proportion of species
not recorded. With this and the number of observed species we
can estimate the total number of species. I did a function
to do this (see scripts at \texttt{functions.R}) and
got the following:

<<table of estimated richness, echo=FALSE>>=
kable(
    data.frame(
        trunc=c("None","0.1/N plots","0.5/N plots","0.9/N plots"),
        S = c(bb.S, bbt.1.S, bbt.5.S, bbt.9.S)),
    digits=0, col.names=c("Truncation","Estimated richness")
    )
@ 

The distribution truncated at $0.9/$\Sexpr{N.plots} gives unrealistic estimates,
so I'll follow with the model with truncation at $0.5/$\Sexpr{N.plots}.
With the likelihood surface we get an idea of likelihood interval:
%I get a likelihood interval for the richness estimated by this model of 13000--22200.

<<beta likelihood surface, echo=FALSE>>=
## Profile
betat.5.prf <- profile(f.betat.5)
betat.5.ci <- confint(betat.5.prf)
# plotprofmle(betat.5.prf)
## Likelihood surface
## Shows that likelihood interval of species richness is ca 9880 - 10280
x <- seq(betat.5.ci[1,1]*.95, betat.5.ci[1,2]*1.05, length=30)
y <- seq(betat.5.ci[2,1]*.95, betat.5.ci[2,2]*1.05, length=30)
Vbb.Sest <- Vectorize(function(s1, s2,...) bb.Sest(shape1=s1, shape2=s2, ...), c("s1", "s2"))
VLL.betat.5 <- Vectorize(LL.betat.5)
z1 <- outer(x, y, VLL.betat.5)
z1 <- z1 - min(z1)
z2 <- outer(x, y, Vbb.Sest, Sobs=S.obs, size=N.plots)
contour(x,y,z1, levels=2, col="red", xlab="shape1", ylab="shape2")
contour(x, y, z2, add=TRUE, col="blue", levels=c(13000,22200) )
contour(x, y, z2, add=TRUE, col="blue", levels=round(bbt.5.S))
@ 

\subsection*{Truncated beta-binomial}

<<fit truncated beta-binomial, echo=FALSE>>=
##start values converted for rho and prob, 
## both bounded between zero and one
## See help(VGAM::betabinom)
mu.st <- cf.st[1]/sum(cf.st)
rho.st <- 1/(1 + sum(cf.st))
### With logit of parameters
## Likelihood function
unlogit <- function(x) exp(x)/(1+exp(x))
logit <- function(x) log(x/(1-x))
LL2.bbt0 <- function(lmu, lrho){
    -sum(dbetabinom.t0(x = Dec2018$N.plots, size = N.plots, mu = unlogit(lmu), rho = unlogit(lrho), log=TRUE))
}
## fit with mle2
beta.bin.ML2 <- bbmle::mle2(LL2.bbt0, start=list(lmu=logit(mu.st), lrho=logit(rho.st)), method="SANN")
beta.bin.ML2 <- bbmle::mle2(LL2.bbt0, start=as.list(coef(beta.bin.ML2)))
bb.mle2 <- unlogit(coef(beta.bin.ML2))
beta.bin.prf2 <- profile(beta.bin.ML2)
bb.ci2 <- confint(beta.bin.prf2)
## Total number of species
bb2.S <- bb.Sest(mu = bb.mle2[1], rho = bb.mle2[2], size = N.plots, Sobs=S.obs)
@ 

Another easy way to get species richness estimates
from the beta-binomial is to fit this distribution
directly to the observed frequencies.
This distribution is zero-truncated to account for
the unobserved species. In this case only the
binomial distribution that composes the beta-binomial
is truncated. To truncate also the beta compounding distribution
we would need Bayesian models.

With this model I got an estimated total richness of
\Sexpr{round(bb2.S)}, with an likelihood interval below:

<<beta-binomial likelihood surface, echo=FALSE>>=
x <- seq(unlogit(bb.ci2[1,1]), unlogit(bb.ci2[1,2]), length=30)
y <- seq(unlogit(bb.ci2[2,1]), unlogit(bb.ci2[2,2]), length=30)
Vbb.Sest <- Vectorize(bb.Sest, c("mu", "rho"))
VLL.bbt0 <- Vectorize(LL.bbt0)
z1 <- outer(x, y, VLL.bbt0)
z1 <- z1 - min(z1)
z2 <- outer(x, y, Vbb.Sest, Sobs=S.obs, size=N.plots)
contour(x,y,z1, levels=2, col="red", xlab="mu", ylab="rho")
contour(x, y, z2, add=TRUE, col="blue", levels=c(9850,10250)) 
contour(x, y, z2, add=TRUE, col="blue", levels=round(bb2.S))
@

\section*{Non-parametric estimates}

Results of the non-parametric indices
available in the package \emph{SPECIES} \citep{SPECIES}.
All around ~5000 species.
%% Only the 'Penalized non-parametric maximum likelihood estimate'
%% from \citet{wang2005} is close the number of species recorded in Amazon. 
%% The other indices estimate  ~5000 species.

<<SPECIES package, eval=TRUE>>=
Ab <- as.data.frame(table(Dec2018$N.ind))
names(Ab) <- c("j", "n_j")
Ab$j <- as.integer(as.character(Ab$j))
##jackknife method
(Sj <- jackknife(Ab,k=5))
##ACE coverage method
(SChao92 <- ChaoLee1992(Ab,t=10, method="all"))
##Chao1984 lower bound estimator
(SChao84 <- chao1984(Ab))
##Chao and Bunge coverage-duplication method
(SChaoB <- ChaoBunge(Ab,t=10))
##Penalized NPMLE method
(SNPMLE <- pnpmle(Ab,t=15))
##Unconditonal NPMLE method
(SUNPMLE <-unpmle(Ab,t=10))
##Poisson-compound Gamma method
(SPG <- pcg(Ab,t=20))
@ 

\section*{Wrap up}

\subsection*{Findings}
\begin{itemize}
\item There is a gap between the estimates, rather than a more uniform spread over the range of values. We have a cluster of estimates around 5000-6000 and other around or above the known number of species.
\item Using a strict approach of selecting the best SAD model with AIC, negative binomial wins. As noted by \citet{tovo2017}, NB provides a lower total richness estimate compared to Log-series. But now it is still a realistic value (\Sexpr{S.nb}). The diagnostic plots show that is so because LS overestimate singletons more than NB. If we go for NB we would check if/how to make the hypothetical SAD for the whole Amazonia from a Negative Binomial, as the codes by Steege et al. do.
\item The realistic estimates (that is, above the known number of species)  
  came from methods that
  approximate some  effect of a large beta-diversity. 
  Such beta-diversity effect is approximated in different ways.
  Log-series assumes an open metacommunity with an infinite number of species,
  negative binomial describes SADs with a variance higher than those expected
  by an independent distribution of species, beta-binomial models
  capture spatial aggregation of species.
\end{itemize}

\subsection*{Open questions}

\begin{itemize}
\item Which models to include?
  \begin{itemize}
  \item I would not use the 1st method to estimate S from Poisson-Lognormal, nor the truncated beta method. Agree?
  \item On the other hand, not so sure if we should include truncated beta-binomial. \textbf{Pros}: raises an simple, alternative way to estimate (occupancy-based); realistic estimates based on some assumption of beta-diversity. \textbf{Cons}: shift the focus from SADs (best keep for another paper?); need to search in the literature to check if/how this method has already been used.
  \end{itemize}
\item Keep the estimation based the reconstruction of the regional SAD? If so, try to do that for negative binomial? If we stick to the current reconstruction based on LS, how to deal with the better performance of NB from model selection?
\end{itemize}

\section*{More (and some redundant) thoughts}

Oldies, kept for the records

I can see a paper that reviews comprehensively the species richness estimates we can get from this data. Of course one important information is the known number of species recorded in the region. There is a gap between the estimates, rather than a more uniform spread over the range of values. We have a cluster of estimates around 5000-6000 and other around the known number of species. Another idea is to do the same for Mata Atlantica.  A nice main figure would be a point-and-error-bars of all estimates for both ecorregions.

The estimates provided by non-parametric and
also the parametric methods with strong assumptions of random and independent dispersion of species
across space (which implies in describing the plots as Poisson samples) were
consistently below the number of species currently recorded for the Amazon.

On the other hand, parametric estimates based on models that
approximates some  effect of a large beta-diversity 
provided consistently larger
values, which encompassed the known number of species. 
Such beta-diversity effect is approximated in different ways.
Log-series assumes an open metacommunity with an infinite number of species,
negative binomial describes SADs with a variance higher than those expected
by an independent distribution of species, and beta and beta-binomial models
capture spatial aggregation of species. 
%% Still to be checked,
%% the only non-parametric estimate that was above 10,000 has also a similar 
%% effect by allowing a different detectability for each species \citep{wang2005}.
I think that the key message here 
is that beta-diversity matters a lot to get a better estimate. And also
that those models that performed better are still rough approximations.

Also, I think that the comparison of the saturation curves of the estimators you did 
for the Amazon is nice. 
Many of them seem to stabilize well below the known number of species, 
while others do show an increasing trend. I think this may be valuable
to discuss the quality of the estimates available for the Amazon. 
I would not venture in general evaluations of the estimators because
there is already a ton of papers about that, so I think we do not
need the simulated data.

%% Finally, I found interesting that
%% some models provided a good fit
%% to a distribution but did not provide a good estimate of species richness.
%% The obvious explanation is that the approximation any model does
%% may break down when you use it in extrapolations. But as obvious it may sound
%% I think it is an important issue concerning the estimation of species richness.


\subsection*{Next steps}

Not sure if for the same paper, but I would proceed with the idea of a model that improves
assumptions about a large beta-diversity. 
My first guess is a minimum model that assumes different degrees
of aggregation for each species. My first choice is a compound Negative binomial-Lognormal
distribution. This is hard to fit by traditional methods, but it is doable with 
simulation and ABC (Approximate Bayesian Computation). To parameterize simulations
we can use the empirical matrix of species x sites. 
I started to work in this idea with Renato and I have some preliminary R codes
of a bootsrap estimator. The \emph{SADISA} package uses a similar approach (using the information of 
replicate samples from the same metacommunity).

A related topic is the idea of SOD's (Species Occupancies Distributions),
which depends of checking if we are not reinventing the weel. But I think
that combining SODs and SADs in a bivariate model has not been fully explored
yet.

\bibliography{/home/paulo/work/resources/bib/geral.bib}

\end{document}
